# AI-Powered Multi-Domain Web Scraper

A modular, extensible system for scraping and analyzing data from multiple domains (Books, Jobs, Real Estate, E-commerce, Education). Data is stored in structured formats (CSV, JSON, Excel, Postgres, MySQL), and Generative AI is integrated for automated insights and report generation.

---

## ğŸ¯ About The Project

This project enables rapid data collection and AI-powered analysis across diverse domains. Its modular design allows you to add new scrapers with minimal effortâ€”just one new file per domain.

---

## âœ¨ Features

- **Multi-domain scraping:** Books, Jobs, Real Estate, E-commerce, Education
- **Structured data storage:** CSV, JSON, Excel, Postgres, MySQL
- **AI-powered analysis:** Automated insights and markdown reports via OpenAI GPT
- **CLI interface:** Simple commands for scraping, analysis, and full pipeline runs
- **Extensible architecture:** Add new scrapers and analysis modules easily
- **Config-driven:** Scraper parameters are set via JSON filesâ€”no code changes needed

---

## ğŸ› ï¸ Tech Stack

- **Python 3.8+**
- **Requests, BeautifulSoup, pandas, SQLAlchemy**
- **OpenAI GPT API**
- **Postgres, MySQL, SQLite (optional)**
- **CLI via argparse**

---

## ğŸ“ Project Structure

```
multi_scraper_ai/
â”‚â”€â”€ data/                       # scraped data storage
â”‚   â”œâ”€â”€ books/
â”‚   â”‚   â””â”€â”€ books_url.json
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â””â”€â”€ jobs_url.json
â”‚   â”œâ”€â”€ real_estate/
â”‚   â”‚   â””â”€â”€ real_estate_url.json
â”‚   â”œâ”€â”€ ecommerce/
â”‚   â”‚   â””â”€â”€ ecommerce_url.json
â”‚   â””â”€â”€ education/
â”‚       â””â”€â”€ education_url.json
â”‚
â”‚â”€â”€ scrapers/                   
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ books_scraper.py        
â”‚   â”œâ”€â”€ jobs_scraper.py         
â”‚   â”œâ”€â”€ real_estate_scraper.py  
â”‚   â”œâ”€â”€ ecommerce_scraper.py    
â”‚   â”œâ”€â”€ education_scraper.py    
â”‚   â””â”€â”€ base_scraper.py         
â”‚
â”‚â”€â”€ analysis/                   
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ books_analysis.py
â”‚   â”œâ”€â”€ jobs_analysis.py
â”‚   â”œâ”€â”€ real_estate_analysis.py
â”‚   â”œâ”€â”€ ecommerce_analysis.py
â”‚   â”œâ”€â”€ education_analysis.py
â”‚   â””â”€â”€ base_analysis.py
â”‚
â”‚â”€â”€ outputs/ 
â”‚   â”œâ”€â”€ report/               
â”‚   â”‚   â”œâ”€â”€ books_report.md
â”‚   â”‚   â”œâ”€â”€ jobs_report.md
â”‚   â”‚   â”œâ”€â”€ real_estate_report.md
â”‚   â”‚   â”œâ”€â”€ ecommerce_report.md
â”‚   â”‚   â””â”€â”€ education_report.md
â”‚   â””â”€â”€ scrapped_data/
â”‚       â”œâ”€â”€ books/
â”‚       â”œâ”€â”€ jobs/
â”‚       â”œâ”€â”€ real_estate/
â”‚       â”œâ”€â”€ ecommerce/
â”‚       â””â”€â”€ education/
â”‚
â”‚â”€â”€ utils/                      
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ db.py                   
â”‚   â”œâ”€â”€ excel_utils.py
â”‚   â”œâ”€â”€ text_cleaner.py
â”‚   â””â”€â”€ config.py
â”‚
â”‚â”€â”€ run.py                      
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ .env
â”‚â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Set up environment variables

```bash
echo "OPENAI_API_KEY=your_openai_api_key_here" > .env
echo "DB_HOST=localhost" >> .env
echo "DB_NAME=scraper_db" >> .env
```

### 3. Create necessary directories

```bash
mkdir -p data/books data/jobs data/real_estate data/ecommerce data/education
mkdir -p outputs/scrapped_data/books outputs/scrapped_data/jobs outputs/scrapped_data/real_estate outputs/scrapped_data/ecommerce outputs/scrapped_data/education
mkdir -p outputs/report
```

---

## ğŸ“± Usage

### Scrape data

```bash
python run.py scrape books --pages 3
python run.py scrape jobs --pages 2
python run.py scrape ecommerce --pages 2
```

### Analyze data with AI

```bash
python run.py analyze books
python run.py analyze jobs
```

### Run complete pipeline

```bash
python run.py run-all jobs --pages 2
```

---

## ğŸ¤– AI Integration

- Each analysis module uses OpenAI GPT to generate insights and markdown reports.
- Reports are saved in `outputs/report/` for each domain.

---

## ğŸŒ Deployment

- Can be run locally or deployed on a server.
- Supports database storage (Postgres/MySQL) for scalable deployments.

---

## ğŸ¨ Customization & Extending

**To add a new domain:**
1. Create a new scraper: `scrapers/{site}_scraper.py`
2. Add an analysis module: `analysis/{site}_analysis.py`
3. Register your scraper in `run.py`
4. Add config/data files in `data/{site}/`

---

## ğŸ“¸ Screenshots

*(Add screenshots of CLI usage, sample reports, and data outputs here)*

---

## ğŸ¤ Contributing

Pull requests and suggestions are welcome! Please open issues for bugs or feature requests.

---

## ğŸ“„ License

MIT License

---

## ğŸ“ Contact

- Project Author: Derrick@cyphertechs
- Portfolio: http://derrickportfolioweb.vercel.app/
- GitHub: https://github.com/derksKCodes

---

## Example Use Cases

- **Books:** â€œWhich genres are cheapest on BooksToScrape right now?â€
- **Jobs:** â€œSummarize the top 5 tech skills from RemoteOK this week.â€
- **Real Estate:** â€œGenerate an AI report on average 3-bedroom house prices in London vs. Manchester.â€
- **E-commerce:** â€œCompare prices of Apple products across Amazon sellers.â€
- **Education:** â€œList the top 5 AI-related Coursera courses and summarize reviews.â€

---

## How It Works

1. **Choose Website:**  
   Run the project with an argument:
   ```bash
   python run.py scrape books
   python run.py site jobs
   python run.py site ecommerce
   ```

2. **Scraper Extracts Data:**  
   - Books â†’ Title, Author, Price, Rating
   - Jobs â†’ Title, Company, Skills, Salary, Location
   - Real Estate â†’ Location, Price, Bedrooms, Agent Contact
   - E-commerce â†’ Product, Price, Rating, Availability
   - Education â†’ Course Name, Instructor, Rating, Duration

3. **AI Analysis:**  
   - Books â†’ Summarize most popular genres
   - Jobs â†’ Identify top 10 skills in demand
   - Real Estate â†’ Compare avg price per location
   - E-commerce â†’ Generate competitor analysis
   - Education â†’ Summarize trending courses

4. **Save Output:**  
   - Data â†’ `data/` (CSV/JSON)
   - AI Insights â†’ `outputs/report/` (Markdown reports)

---

## Extending

To add a new website:
- Create a new scraper in `scrapers/`
- Add an AI analysis script in `analysis/